{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59652d50",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7528e9",
   "metadata": {},
   "source": [
    "We are interested in the state of Michigan, USA at a time period between March 1, 2024, and March 2, 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94af07",
   "metadata": {},
   "source": [
    "Once again, we first load the dataset and improt all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe9d9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversation_hash', 'model', 'timestamp', 'conversation', 'turn', 'language', 'openai_moderation', 'detoxify_moderation', 'toxic', 'redacted', 'state', 'country', 'hashed_ip', 'header'],\n",
       "        num_rows: 837989\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "\n",
    "ds = load_dataset(\"allenai/WildChat-1M\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c57c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d04515",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24e90f",
   "metadata": {},
   "source": [
    "After removing unecessary columns, we filter the dataset to only contain entries from Michigan, and the requested time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97fbb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns\n",
    "ds = ds.remove_columns([\n",
    "    \"conversation_hash\",\n",
    "    \"turn\",\n",
    "    \"language\",\n",
    "    \"openai_moderation\",\n",
    "    \"detoxify_moderation\",\n",
    "    \"toxic\",\n",
    "    \"redacted\",\n",
    "    \"hashed_ip\",\n",
    "    \"header\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f13728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only include entries from Michigan, United States\n",
    "ds = ds.filter(lambda x: x['state'] == 'Michigan' and x['country'] == 'United States') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb781a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['model', 'timestamp', 'conversation', 'state', 'country'],\n",
       "    num_rows: 198\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter to only include entries from March 1, 2024 to March 2, 2024\n",
    "ds = ds.filter(lambda x: x['timestamp'].timetuple()[:3] >= (2024, 3, 1) and x['timestamp'].timetuple()[:3] < (2024, 3, 2))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7bbe4",
   "metadata": {},
   "source": [
    "We then check all the unique models used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cecd3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-3.5-turbo-0125']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.unique('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a7378",
   "metadata": {},
   "source": [
    "The encoding for this model is cl100k_base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d678c30",
   "metadata": {},
   "source": [
    "We create two new columns containing the output and input tokens per conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d023d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b160c1fffb49d9922f2c0404bb18e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['model', 'timestamp', 'conversation', 'state', 'country', 'output_token_count', 'input_token_count'],\n",
       "    num_rows: 198\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize encoder\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# given an example, count the number of output tokens from the assistant and input tokens from the user\n",
    "def count_output_tokens(example):\n",
    "    convo = example['conversation']\n",
    "    example['output_token_count'] = 0\n",
    "    example['input_token_count'] = 0\n",
    "    for turn in convo:\n",
    "        if turn['role'] == 'assistant':\n",
    "            output_text = turn['content']\n",
    "            token_ids = enc.encode(output_text)\n",
    "            example['output_token_count'] += len(token_ids)\n",
    "        elif turn['role'] == 'user':\n",
    "            input_text = turn['content']\n",
    "            token_ids = enc.encode(input_text)\n",
    "            example['input_token_count'] += len(token_ids)\n",
    "    return example\n",
    "\n",
    "# map the function to the dataset\n",
    "ds = ds.map(count_output_tokens)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199e6cb",
   "metadata": {},
   "source": [
    "Given that the maximum amount of input tokens in a multi-turn conversation is only 1403 tokens, we can assume that input tokens do not contribute much more than output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1607868e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1403"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input = max(row['input_token_count'] for row in ds)\n",
    "max_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ee3f2",
   "metadata": {},
   "source": [
    "Adding all tokens together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "264f9175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 153691\n"
     ]
    }
   ],
   "source": [
    "# count total output tokens\n",
    "total_output_tokens = sum(row['output_token_count'] for row in ds)\n",
    "total_input_tokens = sum(row['input_token_count'] for row in ds)\n",
    "total_tokens = total_input_tokens + total_output_tokens\n",
    "\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc24583",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ea75c",
   "metadata": {},
   "source": [
    "## Final Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdaabd6",
   "metadata": {},
   "source": [
    "Given that gpt-3.5-turbo contains 20B parameters, we calculate total FLOPs required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "999e1408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs: 6.15e+15 FLOPs\n"
     ]
    }
   ],
   "source": [
    "# If we assume FLOPs per token is rougly 2 * model_size_in_billions only for inference\n",
    "model_size_in_billions = 20  # for gpt-3.5-turbo, some say 20B others 175B\n",
    "total_flops = 2 * model_size_in_billions * (total_output_tokens + total_input_tokens) * 1e9\n",
    "\n",
    "print(f\"Total FLOPs: {total_flops:.2e} FLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e886ac",
   "metadata": {},
   "source": [
    "Estimating total kWh required for one cluster, given certain hardware characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec629332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total kWh used: 0.1354 kWh\n"
     ]
    }
   ],
   "source": [
    "peak_flops = 9.89e14 # 989 TFLOPS\n",
    "real_flops = peak_flops * 0.1 # 10% efficiency\n",
    "hours_needed = total_flops / real_flops / 3600\n",
    "\n",
    "h100 = 700 * 0.7 # 700W H100 GPU with 70% consumption\n",
    "total_Wh = hours_needed * h100 \n",
    "total_kWh_one_gpu = total_Wh / 1000\n",
    "total_kWh_cluster = total_kWh_one_gpu * 16  # assuming 16 GPUs\n",
    "print(f\"Total kWh used: {total_kWh_cluster:.4f} kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a71204",
   "metadata": {},
   "source": [
    "Finally, calculating the total emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "820ceb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emissions: 0.0596 kgCO2eq\n"
     ]
    }
   ],
   "source": [
    "PUE = 1.1 # data center Power Usage Effectiveness\n",
    "CI = 0.4 # carbon intensity in kgCO2eq/kWh approximate for US in 2024\n",
    "total_emissions_kg = total_kWh_cluster * PUE * CI\n",
    "\n",
    "print(f\"Total emissions: {total_emissions_kg:.4f} kgCO2eq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c306155b",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
